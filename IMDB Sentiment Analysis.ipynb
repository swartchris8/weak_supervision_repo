{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f5142f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import skweak\n",
    "from skweak import utils\n",
    "\n",
    "from typing import Iterable, Optional, Sequence, Tuple\n",
    "\n",
    "from spacy.tokens import Doc, Span  # type: ignore\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e050257",
   "metadata": {},
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a7490",
   "metadata": {},
   "source": [
    "### Load imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a851b649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/chris/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f4a35765d548ae858c07effe528787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f2d98",
   "metadata": {},
   "source": [
    "### Dataset labels\n",
    "\n",
    "0 -- Negative -- 50%\n",
    "\n",
    "1 -- Positive -- 50%\n",
    "\n",
    "-1 -- Unlabelled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "881076eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model_name = \"en_core_web_sm\" # needed for DocBin -> Doc process\n",
    "\n",
    "nlp = spacy.load(spacy_model_name)   # We load an English-language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39514c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889e94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_prep(data_dict):\n",
    "    doc = nlp(data_dict[\"text\"])\n",
    "    doc.user_data[\"gold\"] = data_dict[\"label\"]\n",
    "    return doc\n",
    "    \n",
    "def write_dataset_to_docbin(dataset_chunk, docbin, docbin_path):\n",
    "    for data_dict in tqdm(dataset_chunk):\n",
    "        docbin.add(spacy_prep(data_dict))\n",
    "    if docbin_path:\n",
    "        docbin.to_disk(docbin_path)\n",
    "        \n",
    "def write_docs_to_docbin(docs, docbin_path):\n",
    "    docbin = DocBin(store_user_data=True)\n",
    "    for doc in docs:\n",
    "        docbin.add(doc)\n",
    "    docbin.to_disk(docbin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efcf1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_doc_bin = DocBin(store_user_data=True)\n",
    "# unsupervised_doc_bin = DocBin(store_user_data=True)\n",
    "# test_doc_bin = DocBin(store_user_data=True)\n",
    "\n",
    "\n",
    "# write_dataset_to_docbin(dataset[\"train\"], train_doc_bin, \"sentiment_docbin/train.docbin\")\n",
    "\n",
    "# write_dataset_to_docbin(dataset[\"unsupervised\"], unsupervised_doc_bin, \"sentiment_docbin/unsupervised.docbin\")\n",
    "\n",
    "# write_dataset_to_docbin(dataset[\"test\"], test_doc_bin, \"sentiment_docbin/test.docbin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b154e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = list(utils.docbin_reader(\"sentiment_docbin/train.docbin\", spacy_model_name=spacy_model_name))\n",
    "\n",
    "test_docs = list(utils.docbin_reader(\"sentiment_docbin/test.docbin\", spacy_model_name=spacy_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3531763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_labels(docs):\n",
    "    return [doc.user_data[\"gold\"] for doc in docs]\n",
    "\n",
    "train_true = gold_labels(train_docs)\n",
    "test_true = gold_labels(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7c864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00217f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup positive mock annotator\n",
      "Setup negative mock annotator\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Optional, Sequence, Tuple\n",
    "\n",
    "from spacy.tokens import Doc, Span  # type: ignore\n",
    "\n",
    "class MockPositiveAnnotator(skweak.base.SpanAnnotator):\n",
    "    \"\"\"Annotate everything positive\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        name = \"mock_positive_annotator\"\n",
    "        super(MockPositiveAnnotator, self).__init__(name)\n",
    "\n",
    "        print(\"Setup positive mock annotator\")\n",
    "\n",
    "    def find_spans(self, doc: Doc) -> Iterable[Tuple[int, int, str]]:\n",
    "        yield 0, len(doc), \"1\"\n",
    "        \n",
    "        \n",
    "class MockNegativeAnnotator(skweak.base.SpanAnnotator):\n",
    "    \"\"\"Annotate everything negative\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        name = \"mock_negative_annotator\"\n",
    "        super(MockNegativeAnnotator, self).__init__(name)\n",
    "\n",
    "        print(\"Setup negative mock annotator\")\n",
    "\n",
    "    def find_spans(self, doc: Doc) -> Iterable[Tuple[int, int, str]]:\n",
    "        yield 0, len(doc), \"0\"\n",
    "        \n",
    "mock_positive_annotator = MockPositiveAnnotator()\n",
    "mock_negative_annotator = MockNegativeAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf98205",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_voter = skweak.voting.SequentialMajorityVoter(\"maj_voter\", labels=[\"0\", \"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32648716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<skweak.base.CombinedAnnotator at 0x7faf00645060>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi_mock = skweak.base.CombinedAnnotator()\n",
    "combi_mock.add_annotator(mock_positive_annotator)\n",
    "combi_mock.add_annotator(mock_negative_annotator)\n",
    "combi_mock.add_annotator(majority_voter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a20694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53646689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock_train_results = list(combi_mock.pipe(train_docs))\n",
    "# mock_test_results = list(combi_mock.pipe(train_docs))\n",
    "\n",
    "# write_docs_to_docbin(mock_train_results, \"mock_train.docbin\")\n",
    "# write_docs_to_docbin(mock_train_results, \"mock_test.docbin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d06a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_train_results = list(utils.docbin_reader(\"mock_train.docbin\", spacy_model_name=spacy_model_name))\n",
    "\n",
    "mock_test_results = list(utils.docbin_reader(\"mock_test.docbin\", spacy_model_name=spacy_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f78940b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5dda227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_train_results[0].spans[\"mock_positive_annotator\"][0].label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b022c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_labels(docs, agg_name=\"maj_voter\"):\n",
    "    return [int(doc.spans[agg_name][0].label_) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d4b7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_train_labels = predicted_labels(mock_train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4fba5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_test_labels = predicted_labels(mock_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "033f27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    print(f\"\"\"Precision {precision_score(y_true, y_pred)},\n",
    "    Recall {recall_score(y_true, y_pred)},\n",
    "    F1 {f1_score(y_true, y_pred)}\n",
    "    \n",
    "    ---\n",
    "    Confusion matrix\n",
    "    {confusion_matrix(y_true, y_pred)}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7912fac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.0,\n",
      "    Recall 0.0,\n",
      "    F1 0.0\n",
      "    \n",
      "    ---\n",
      "    Confusion matrix\n",
      "    [[12500     0]\n",
      " [12500     0]]\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/skweak/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(train_true, mock_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89cfe53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.0,\n",
      "    Recall 0.0,\n",
      "    F1 0.0\n",
      "    \n",
      "    ---\n",
      "    Confusion matrix\n",
      "    [[12500     0]\n",
      " [12500     0]]\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/skweak/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(test_true, mock_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "646a2ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/skweak/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "y_true = [0, 1, 1, 0, 1, 1]\n",
    "y_pred = [0, 0, 0, 0, 0, 0]\n",
    "precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee23b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6538b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skweak import gazetteers\n",
    "\n",
    "NEGATIVE_WORDS = [\"unhappy\", \"sad\", \"disappointing\", \"bad\"]\n",
    "NEGATIVE_WORDS = [[word] for word in NEGATIVE_WORDS]\n",
    "\n",
    "\n",
    "POSITIVE_WORDS = [\"happy\", \"great\", \"awesome\", \"amazing\", \"good\", \"fun\"]\n",
    "POSITIVE_WORDS = [[word] for word in POSITIVE_WORDS]\n",
    "# NEGATIVE_WORDS = [\"unhappy\", ]\n",
    "\n",
    "positive_trie = gazetteers.Trie(POSITIVE_WORDS)\n",
    "\n",
    "gazetteer = gazetteers.GazetteerAnnotator(\"sent_gazetteer\", {1:positive_trie})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac0eb755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GazetteerDocDetector(skweak.base.SpanAnnotator):\n",
    "    def __init__(self):\n",
    "        super(GazetteerDocDetector, self).__init__(\"gazetteer_doc_detector\")\n",
    "\n",
    "    def find_spans(self, doc):\n",
    "        gazetteer_score = 0\n",
    "        for start, end, score in gazetteer.find_spans(doc):\n",
    "            gazetteer_score += score \n",
    "        if 0 < gazetteer_score:\n",
    "            yield 0, len(doc), \"1\" # type: ignore\n",
    "        # TODO set default to -1 (unlabelled) or 0 (negative)?\n",
    "        else:\n",
    "            yield 0, len(doc), \"0\" # type: ignore\n",
    "        \n",
    "gazetteer_doc_detector = GazetteerDocDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e39051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_voter = skweak.voting.SequentialMajorityVoter(\"maj_voter\", labels=[\"0\", \"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e636c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<skweak.base.CombinedAnnotator at 0x7faedf6ddff0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sentiment = skweak.base.CombinedAnnotator()\n",
    "combined_sentiment.add_annotator(gazetteer_doc_detector)\n",
    "\n",
    "combined_sentiment.add_annotator(majority_voter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf97c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaz_train_docs = list(combined_sentiment.pipe(train_docs))\n",
    "# write_docs_to_docbin(gaz_train_docs, \"trained_v1.docbin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ab6f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_docs_test = list(combined_sentiment.pipe(test_docs))\n",
    "write_docs_to_docbin(gaz_docs_test, \"gaz_docs_test.docbin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bc63a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_docs_train = list(utils.docbin_reader(\"trained_v1.docbin\", spacy_model_name=spacy_model_name))\n",
    "\n",
    "gaz_docs_test = list(utils.docbin_reader(\"gaz_docs_test.docbin\", spacy_model_name=spacy_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba2451e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_docs_train_labels = predicted_labels(gaz_docs_train)\n",
    "gaz_docs_test_labels = predicted_labels(gaz_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b1645d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.5524544179523142,\n",
      "    Recall 0.63024,\n",
      "    F1 0.5887892376681615\n",
      "    \n",
      "    ---\n",
      "    Confusion matrix\n",
      "    [[6118 6382]\n",
      " [4622 7878]]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(train_true, gaz_docs_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8745e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.5522292083452415,\n",
      "    Recall 0.61832,\n",
      "    F1 0.5834088164251208\n",
      "    \n",
      "    ---\n",
      "    Confusion matrix\n",
      "    [[6233 6267]\n",
      " [4771 7729]]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(train_true, gaz_docs_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb87d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587391f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
